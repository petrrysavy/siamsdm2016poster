\documentclass[25pt, a0paper, landscape, margin=0mm, innermargin=15mm, blockverticalspace=15mm, colspace=15mm, subcolspace=8mm]{tikzposter}

\title{Geometric Methods of Accelerating Triangle-inequality-based $k$-means}
\author{Petr Ry\v{s}av\'{y} and Greg Hamerly}
\institute{Baylor University}

\usetheme{Desert}
\usecolorstyle[colorPalette=GreenGrayViolet,colorOne=green!50!black,colorTwo=white,colorThree=black]{Denmark}
\colorlet{titlefgcolor}{black}

\usepackage{files/mycommands}

\begin{document}
\maketitle
\begin{columns}
\column{0.25}

\block[titleoffsety=-100cm,bodyoffsety=-100cm]{}{
Petr Ry\v{s}av\'{y} and Greg Hamerly \\
Department of Computer Science\\
Baylor University\\
Waco, TX 76798-7356 \\
\href{mailto:petr_rysavy@alumni.baylor.edu }{petr\_rysavy@alumni.baylor.edu } \\
\href{mailto:greg_hamerly@baylor.edu}{greg\_hamerly@baylor.edu}
}

\block{Abstract}{
\textit{Most implementations of $k$-means use Lloyd's algorithm, which does many unnecessary distance calculations. Several accelerated algorithms produce exactly the same answer. They avoid redundant work using the triangle inequality paired with a set of bounds on point-centroid distances. Our proposals allow those algorithms to perform even better, giving up to eight times further speedup. Our methods give tighter lower bound updates and efficiently skip centroids that cannot possibly be close to a set of points.}
}


\block{$k$-means problem}{
  Input
  \begin{itemize}
    \item A set of points $\left\{ \itemization{\vec{x}}{n} \right\}$.
    \item Number of centroids $k$.
  \end{itemize}
  Goal is to find to find a set of $k$ points $\left\{ \itemization{\vec{c}}{k} \right\}$, named \emph{centroids},
  that minimize the \emph{distortion function}
  \begin{equation*}
    J(\itemization{\vec{c}}{k}, \vec{c}) = \sumion \left\| \vec{x}_i - \vec{c}(\vec{x}_i) \right\|^2.
    \label{eq:distortion}
  \end{equation*}
  %(function $\vec{c}$ returns the assigned centroid to the given argument)
  \input{files/clusters}
}

\block{Lloyd's algorithm \cite{lloyd}}{
  Iteratively repeats two steps:
     \begin{enumerate}
       \item Assign each point to its closest centroid
       \item Move centroids to the cluster means
     \end{enumerate}
}


\column{0.25}

\block{Triangle-inequality-based $k$-means}{
   Lloyd's algorithm does many redundant distance calculations.
   If a centroid does not move or the movement is small, the
   distances are recalculated. There are several algorithms that
   target this inefficiency by maintaining a set of upper and lower bounds.
   Those bounds are guaranteed by the triangle inequality.
   Common property of those algorithms is that they produce
   \emph{exactly} the same result as Lloyd's algorithm,
   only \emph{faster}.
   
  \begin{theorem}[Triangle inequality] \label{thm:triange}
    For any vectors $\vec{x}$ and $\vec{y}$ holds
    \begin{equation*}
       \| \vec{x} + \vec{y} \| \leq \|\vec{x}\| + \|\vec{y}\|.
    \end{equation*}
 \end{theorem}
 \begin{center}
 \tiny
  \input{img/triangle1.pgf}
  \input{img/triangle2.pgf}
 \end{center}
}

\block{Elkan's algorithm \cite{elkan}}{
  \begin{itemize}
    \item One \emph{upper bound} $\ux$ on the distance to the closest centroid.
    \item $k$ \emph{lower bounds} $\lxcj$ on the distances between the
      point $\x$ and each centroid.
  \end{itemize}
  \begin{center}
    \input{img/elkanbounds.pgf}
  \end{center}
}

\block{Hamerly's algorithm \cite{hamerly}}{
  \begin{itemize}
    \item One \emph{upper bound} $u(\vec{x})$.
    \item One \emph{lower bound} $l(\vec{x})$ that stands for a lower bound on the distance
      between the point $\vec{x}$ and its second closest centroid.
  \end{itemize}
  \begin{center}
    \input{img/hamerlybounds.pgf}
  \end{center}
}

\column{0.25}

\block{Tighter update}{
    We assume the worst case while we update the upper/lower bounds.
    Points in cluster are not everywhere in space, but
    they fulfill some locality. When a centroid $\cj$
    moves away from a point $\x$,
    the lower bound $\lxcj$ does not have to shrink.
  
    We use the upper bound to localize the cluster. 
    Any point is at most $\ux$ from its closest centroid.
    Therefore each point is at most
      \begin{equation*}
         \mci = \max_{\vec{y} \mid \vec{c}(\vec{y}) = \ci} u(\vec{y}).
      \end{equation*}
      from its closest centroid $\ci$.
  
     The update $\deltaxcj$ of $\lxcj$ must fulfill
       \begin{equation*}
         \deltaxcj \geq \distxcj - \distxcjp.
       \end{equation*}
     I.e. the update is at least the difference between
     the old and the new distance. Now focus on function
      $\distxcj - \distxcjp = f(\x)$.
}

\block{Lower bound update calculation}{
If we fix $f(x) = \distxcj - \distxcjp = z$, we obtain a hyperbola. For any point on or above the hyperbola we can use $\deltaxcj = z$ as the update of the lower bound $\lxcj$.

The optimal value of $z$ could be found if the hyperbola touches the sphere that contains the cluster. This would lead to too costly calculations. Therefore instead we let asymptote of the hyperbola touch the sphere and use a bit bigger value of $z$.

\begin{lemma}
Suppose that $\x \in \R^2$, $\ux \leq r \in \Rpz$ and $\ci = (c_{ix}, c_{iy})$,
where $c_{ix} > r$ and $c_{iy} \leq r$. Let $\cj=(0,1)$ and $\cj'=(0,-1)$. Then
\begin{equation*}
    \deltaxcj =
        2\frac{
            c_{ix} r
            -
            c_{iy} \sqrt{\| \ci \|^2 - r^2}
        }{
             \| \ci \|^2
        }
\end{equation*}
is a valid update of the lower bound $\lxcj$.
\end{lemma}
  \begin{center}
    \input{img/situation.pgf}
  \end{center}
}

\column{0.25}
\block{Avoiding $\Theta(n)$ work in the innermost loop}{
    In Hamerly's algorithm we need to know the distance to
    the second closest centroid in the innermost loop.
    What if we know which centroid is the second closest?
    If we know the closest and the second closest centroid,
    we do not need to calculate the distances to the other centroids.
  \begin{definition}
    A \emph{neighbor} of a centroid $\ci$ is any centroid $\cj$
    that is closest or second closest to any point assigned to $\ci$.
  \end{definition}
  \begin{theorem}
    Any neighbor $\cj$ of a centroid $\ci$ must fulfill the condition
    \begin{equation}
      m(\ci) + s(\ci) \geq \frac{1}{2} \| \ci - \cj \|.
      \label{eq:neighborcond}
    \end{equation}
  \end{theorem}
  $\sci$ is the distance from $\ci$ to closest other centroid, i.e.
  \begin{equation}
    \sci = \frac{1}{2} \min_{j \in \{1,2, \ldots, i-1, i+1, \ldots, k\}} \| \cj - \ci \|.
  \end{equation}
    If the condition \eqref{eq:neighborcond} is violated,
    we know that for any $\x$ assigned to $\ci$:
    \begin{itemize}
      \item $\ci$ is closer to $\x$ than $\cj$ and
      \item the closest other centroid to $\ci$ is closer to $\x$ than $\cj$.
    \end{itemize}
    Therefore we do not need to consider $\cj$ in the innermost
    loop for \emph{all} points in the cluster assigned to $\ci$.
}

\block{Experimental results}{
 Sources are available at \url{https://github.com/petrrysavy/baylorml} [branches \texttt{modified\_update} and \texttt{multithreaded}]. Implementation is based on code by G. Hamerly and J. Drake. We tested the algorithms on synthetic and real-world datasets.
 
 Changes require operations per iteration and pair of centroids. There is no additional work per point. Therefore there is low risk of slowing down the algorithm while the runtime may improve several times. Changes work better in lower dimension and when data contain a natural clustering. Algorithms become up to 8 times faster that the original versions and 300 times faster than Lloyd's algorithm.
}

\block{References}{
    \scriptsize
	\nocite{chapter, hamerly, elkan, lloyd}
    \bibliographystyle{plain}
    \bibliography{files/thesis}
}

\end{columns}

\end{document}