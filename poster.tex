\documentclass[25pt, a0paper, landscape, margin=0mm, innermargin=15mm, blockverticalspace=15mm, colspace=15mm, subcolspace=8mm]{tikzposter}

\title{Geometric Methods of Accelerating Triangle-inequality-based $k$-means}
\author{Petr Ry\v{s}av\'{y} and Greg Hamerly}
\institute{Baylor University, Waco, TX}

\usetheme{Desert}
\usecolorstyle[colorPalette=GreenGrayViolet,colorOne=green!30!black,colorTwo=white,colorThree=black]{Denmark}
\colorlet{titlefgcolor}{black}
\colorlet{titlebgcolor}{white}

\usepackage{files/mycommands}

\begin{document}
\maketitle
\begin{columns}
\column{0.25}

\block[titleoffsety=-100cm,bodyoffsety=-100cm]{}{
Petr Ry\v{s}av\'{y} and Greg Hamerly \\
Department of Computer Science\\
Baylor University\\
Waco, TX 76798-7356 \\
\href{mailto:petr_rysavy@alumni.baylor.edu }{petr\_rysavy@alumni.baylor.edu } \\
\href{mailto:greg_hamerly@baylor.edu}{greg\_hamerly@baylor.edu}
}

\block{Abstract}{
\textit{Most implementations of $k$-means use Lloyd's algorithm, which does {\bf many unnecessary distance calculations}. Several accelerated algorithms produce exactly the same answer by using bounds on point-center distances which are efficiently updated with the triangle inequality.  In this work we propose {\bf tighter lower bound updates} and {\bf efficiently skip centroids} that cannot possibly be close to a set of points. In our experiments, these improvements {\bf accelerate fast algorithms up to eight times faster}.}
}


\block{$k$-means Clustering}{
    \begin{itemize}
    \item Inputs: $n$ points $\left\{ \itemization{\vec{x}}{n} \right\}$;
    number of centroids $k$.
    \item Goal: find a set of $k$ centroids $\left\{ \itemization{\vec{c}}{k}
    \right\}$ to minimize the \emph{distortion function}
      \begin{equation*}
        J(\itemization{\vec{c}}{k}) = \sumion \min_{1 \le j \le k} \left\| \vec{x}_i - \vec{c}_j \right\|^2.
        \label{eq:distortion}
      \end{equation*}
    \end{itemize}
  %(function $\vec{c}$ returns the assigned centroid to the given argument)
  \input{files/clusters}
}

\block{Lloyd's Algorithm \cite{lloyd}}{
  Repeat until convergence:
     \begin{enumerate}
       \item {\bf Assign} each point to its closest centroid
       \item {\bf Move} centroids to the cluster means
     \end{enumerate}
}


\column{0.25}

\block{Triangle-Inequality-Based $k$-means}{
   Lloyd's algorithm calculates all point-center distances, even when centers
   move very little. Several algorithms avoid this redundant work by
   maintaining distance bounds which can prove when cluster assignments could
   not have changed.  The triangle inequality allows efficient and correct but
   loose bound updates.  Such algorithms produce \emph{exactly} the same
   results as Lloyd's algorithm, only \emph{faster}.
   
  \begin{theorem}[Triangle inequality] \label{thm:triange}
    For any vectors $\vec{x}$ and $\vec{y}$,
    \begin{equation*}
       \| \vec{x} + \vec{y} \| \leq \|\vec{x}\| + \|\vec{y}\|.
    \end{equation*}
 \end{theorem}
 \begin{center}
 \tiny
  \input{img/triangle1.pgf}
  \input{img/triangle2.pgf}
 \end{center}
}

\block{Elkan's Algorithm \cite{elkan}}{
  \begin{itemize}
    \item One \emph{upper bound} $\ux$ -- distance to the assigned centroid.
    \item $k$ \emph{lower bounds} $\lxcj$ -- distance to each centroid.
  \end{itemize}
  \begin{center}
    \input{img/elkanbounds.pgf}
  \end{center}
}

\block{Hamerly's Algorithm \cite{hamerly}}{
  2 point-center distance bounds per point:
  \begin{itemize}
    \item One \emph{upper bound} $u(\vec{x})$.
    \item One \emph{lower bound} $l(\vec{x})$ -- distance
      to the second-closest centroid.
  \end{itemize}
  \begin{center}
    \input{img/hamerlybounds.pgf}
  \end{center}
}

\column{0.25}

\block{Idea 1: Tighter Bound Updates}{
    The triangle inequality \cite{elkan, hamerly} is a worst-case update for
    the bounds. Points have locality, so when $\cj$ moves {\em away} from $\x$, the
    lower bound $\lxcj$ need not shrink. If the lower bound decreases
    slower, the bound is more useful.
  
    We use the upper bound to localize the cluster. 
    %Any point is at most $\ux$ from its closest centroid.
    Each point is at most
      \begin{equation*}
         \mci = \max_{\vec{x} \mid \vec{c}(\vec{x}) = \ci} u(\vec{x}).
      \end{equation*}
      from its closest centroid $\ci$.
  
     The update $\deltaxcj$ of $\lxcj$ must fulfill
       \begin{equation*}
         \deltaxcj \geq \distxcj - \distxcjp.
       \end{equation*}
     I.e. the update is at least the difference between
     the old and the new distance. Now focus on function
      $\distxcj - \distxcjp = f(\x)$.
}

\block{Lower Bound Update Calculation}{
If we fix $f(x) = \distxcj - \distxcjp = z$, we obtain a hyperbola. For any point on or above the hyperbola we can use $\deltaxcj = z$ as the update of the lower bound $\lxcj$.

The optimal value of $z$ could be found if the hyperbola touches the sphere that contains the cluster. This would lead to too costly calculations. Therefore instead we let asymptote of the hyperbola touch the sphere and use a bit bigger value of $z$.

\begin{lemma}
Suppose that $\x \in \R^2$, $\ux \leq r \in \Rpz$ and $\ci = (c_{ix}, c_{iy})$,
where $c_{ix} > r$ and $c_{iy} \leq r$. Let $\cj=(0,1)$ and $\cj'=(0,-1)$. Then
\begin{equation*}
    \deltaxcj =
        2\frac{
            c_{ix} r
            -
            c_{iy} \sqrt{\| \ci \|^2 - r^2}
        }{
             \| \ci \|^2
        }
\end{equation*}
is a valid update of the lower bound $\lxcj$.
\end{lemma}
  \begin{center}
    \input{img/situation.pgf}
  \end{center}
}


% Hamerly left off editing around here Fri Apr 29 20:55:13 UTC 2016

\column{0.25}
\block{Idea 2: Avoid $\Theta(k)$ Work in the Innermost Loop}{
    In Hamerly's algorithm we need to know the distance to
    the second closest centroid in the innermost loop.
    If we knew the closest and the second closest centroid,
    we would not need to calculate the distances to the other centroids.
  \begin{definition}
    A \emph{neighbor} of a centroid $\ci$ is any centroid $\cj$
    that is closest or second closest to any point assigned to~$\ci$.
  \end{definition}
  \begin{theorem}
    Any neighbor $\cj$ of a centroid $\ci$ must fulfill the condition
    \begin{equation}
      m(\ci) + s(\ci) \geq \frac{1}{2} \| \ci - \cj \|.
      \label{eq:neighborcond}
    \end{equation}
  \end{theorem}
  $\sci$ is the distance from $\ci$ to closest other centroid.
  
    If \eqref{eq:neighborcond} is violated,
    we know that for any $\x$ assigned to $\ci$:
    \begin{itemize}
      \item $\ci$ is closer to $\x$ than $\cj$ and
      \item the closest other centroid to $\ci$ is closer to $\x$ than $\cj$.
    \end{itemize}
    Therefore we do not need to consider $\cj$ in the innermost
    loop for \emph{all} points in the cluster assigned to $\ci$.
}

\block{Experimental results}{
 Sources are available at \url{https://github.com/petrrysavy/baylorml} [branches \texttt{modified\_update} and \texttt{multithreaded}]. Implementation is based on code by G. Hamerly and J. Drake. We tested the algorithms on synthetic and real-world datasets.
 
 Changes require operations per iteration and pair of centroids. There is no additional work per point. Therefore there is low risk of slowing down the algorithm while the runtime may improve several times. Changes work better in lower dimension and when data contain a natural clustering. Algorithms become up to 8 times faster that the original versions and 300 times faster than Lloyd's algorithm.
}

\block{References}{
    \footnotesize
	\nocite{chapter, hamerly, elkan, lloyd}
    \bibliographystyle{plain}
    \bibliography{files/thesis}
}

\end{columns}

\end{document}
